<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>MLP Documentation</title>
    <script type="text/javascript" async src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/3.2.0/es5/tex-mml-chtml.js"></script>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 20px;
            background-color: #f4f4f4;
        }
        h1, h2 {
            text-align: center;
            color: #333;
        }
        h1 {
            font-size: 2.5em;
            margin-bottom: 10px;
        }
        h2 {
            font-size: 1.8em;
            margin-bottom: 20px;
        }
        p {
            font-size: 1.1em;
            color: #555;
            margin-bottom: 15px;
        }
        ul {
            margin: 20px 0;
            padding-left: 20px;
        }
        ul li {
            margin-bottom: 10px;
            font-size: 1.1em;
        }
        img {
            display: block;
            margin: 20px auto;
            border-radius: 8px;
            box-shadow: 0 4px 10px rgba(0, 0, 0, 0.1);
        }
        a {
            color: #2a9df4;
            text-decoration: none;
        }
        a:hover {
            text-decoration: underline;
        }
        .formula {
            background-color: #fff;
            padding: 10px;
            border-left: 4px solid #2a9df4;
            margin: 20px 0;
            border-radius: 5px;
        }
        footer {
            text-align: center;
            margin-top: 50px;
            color: #888;
        }
    </style>
</head>
<body>

    <h1>Multi Layer Perceptron (MLP)</h1>

    <h2>Introduction</h2>
    <p>As you know, some algorithms such as linear regression, logistic regression, softmax classification, or Perceptron learning algorithms (PLA) require classes to be linearly separable (which is difficult in real life). To solve this, a multi-layer perceptron is used.</p>

    <h2>Problem Description</h2>
    <p>Suppose you have datasets like this:</p>
    <img src="Image/EX.png" width="800"/>

    <p>The classes are not linearly separable, so we will use MLP to tackle this.</p>
    <img src="Image/HiddenLayer.png" width="800"/>

    <h2>Behind the scenes</h2>
    <img src="Image/MoreDetailHiddenlayer.png" width="800"/>

    <p>Note: <b>W</b><sup>(l)</sup> are the weights...</p>

    <p>The derivative of the loss function with respect to a single component:</p>
    <div class="formula">
        \[
        \frac{\partial J}{\partial w_{ij}^{(L)}}
        = \frac{\partial J}{\partial z_j^{(L)}} \times \frac{\partial z_j^{(L)}}{\partial w_{ij}^{(L)}}
        = e_j^{(L)} a_i^{(L-1)} \quad (1)
        \]
    </div>

    <p>where \( e_j^{(L)} \) is defined as:</p>
    <div class="formula">
        \[
        e_j^{(L)} = \frac{\partial J}{\partial z_j^{(L)}}
        \]
    </div>

    <p>Continue with (1):</p>
    <div class="formula">
        \[
        \frac{\partial J}{\partial w_{ij}^{(l)}}
        = \frac{\partial J}{\partial z_j^{(l)}} \cdot \frac{\partial z_j^{(l)}}{\partial w_{ij}^{(l)}}
        = e_j^{(l)} a_i^{(l-1)}
        \]
    </div>

    <!-- Add more content as needed -->

    <h2>Result</h2>
    <img src="Image/Result.png" width="800"/>

    <h2>Requirements</h2>
    <ul>
        <li>Python</li>
        <li>Softmax regression</li>
        <li>CrossEntropy</li>
        <li>Gradient Descent</li>
    </ul>

    <h2>References</h2>
    <ul>
        <li><a href="https://machinelearningcoban.com/2017/02/24/mlp/">Machine Learning Cơ Bản</a></li>
        <li><a href="https://cs231n.github.io/neural-networks-case-study/">CS231n Neural Networks Case Study</a></li>
    </ul>

    <footer>
        <p>MLP Documentation | &copy; 2025</p>
    </footer>

</body>
</html>
